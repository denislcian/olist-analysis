{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM8FQe7f6y0b"
      },
      "source": [
        "## Paso 0: Acceso a Datos \n",
        "Para garantizar la persistencia de los datos y simular un entorno de producci√≥n donde los datos residen en la nube, importamos los datos desde la carpeta data/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bol_S3Nw6yYp",
        "outputId": "5a72204c-1254-4f04-c7e0-1b7140545430"
      },
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as\n",
        "import pandas as pd \n",
        "import os\n",
        "\n",
        "# Configuraci√≥n de rutas \n",
        "DATA_PATH = \"../data/\"\n",
        "\n",
        "# Diccionario de archivos \n",
        "files = {\n",
        "    \"customers\": \"olist_customers_dataset.csv\",\n",
        "    \"items\": \"olist_order_items_dataset.csv\",\n",
        "    \"orders\": \"olist_orders_dataset.csv\",\n",
        "    \"products\": \"olist_products_dataset.csv\",\n",
        "    \"translation\": \"product_category_name_translation.csv\"\n",
        "}\n",
        "\n",
        "def load_olist_data(directory, file_map):\n",
        "    \"\"\"Carga los datasets y verifica su existencia.\"\"\"\n",
        "    loaded_data = {}\n",
        "    for key, name in file_map.items():\n",
        "        full_path = os.path.join(directory, name)\n",
        "        if os.path.exists(full_path):\n",
        "            loaded_data[key] = pd.read_csv(full_path)\n",
        "            print(f\"{name} cargado correctamente.\")\n",
        "        else:\n",
        "            print(f\"Error: No se encuentra {name} en la ruta {directory}\")\n",
        "    return loaded_data\n",
        "\n",
        "# Ejecuci√≥n de la carga\n",
        "datasets = load_olist_data(DATA_PATH, files)\n",
        "\n",
        "# Asignaci√≥n de variables globales para el an√°lisis\n",
        "df_customers = datasets.get(\"customers\")\n",
        "df_items = datasets.get(\"items\")\n",
        "df_orders = datasets.get(\"orders\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkxvvIrMmX_9"
      },
      "source": [
        "# Data Analytics & Segmentaci√≥n IA - Olist E-commerce\n",
        "**AUTOR:** DENISLMO\n",
        "**DATASET:** Olist Brazil (Kaggle)\n",
        "\n",
        "## Paso 1: Configuraci√≥n de entorno\n",
        "En esta secci√≥n improtamos las librer√≠as necesarias y preparamos un motor **SQL (SQLite)**\n",
        "Utilizamos SQL para la extracci√≥n de datos porque es m√°s eficiente en entornos productivos que cargan archivos masivos directamente en memoria en Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfavBgVRkpu9",
        "outputId": "8cc2b3d4-f6b3-4776-8097-aa57b94180c9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns \n",
        "\n",
        "# Estetica gr√°fica\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "\n",
        "# 1 Carga de datos - Creamos funci√≥n\n",
        "def load_olist_data(file_name):\n",
        "  return pd.read_csv(os.path.join(DATA_PATH, file_name))\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "# Cargamos solo lo necesario para este hito\n",
        "  orders = load_olist_data('olist_orders_dataset.csv')\n",
        "  items = load_olist_data('olist_order_items_dataset.csv')\n",
        "  products = load_olist_data('olist_products_dataset.csv')\n",
        "  customers = load_olist_data('olist_customers_dataset.csv')\n",
        "\n",
        "  # Esta tabla es la clave para traducir las categor√≠as\n",
        "  translation = load_olist_data('product_category_name_translation.csv')\n",
        "\n",
        "  # 2 Conexi√≥n a motor SQL en memoria\n",
        "  conn = sqlite3.connect(':memory:')\n",
        "\n",
        "  # 3 Transferencia a SQL\n",
        "\n",
        "  orders.to_sql('orders', conn, index=False)\n",
        "  items.to_sql('items', conn, index=False)\n",
        "  products.to_sql('products', conn, index=False)\n",
        "  translation.to_sql('translation', conn, index=False)\n",
        "  customers.to_sql('customers', conn, index=False)\n",
        "\n",
        "  print(\"Datos cargados y motor SQL activo\")\n",
        "except Exception as e:\n",
        "  print(f\"Error al cargar los datos: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibUTfg_oAQh4"
      },
      "source": [
        "## Paso 2: Identificaci√≥n de Categor√≠as Estrat√©gicas\n",
        "¬øDonde genera Olist su dinero? Antes de aplicar modelos de ML, necesitamos entender que productos tienen el mayor impacto financiero.\n",
        "\n",
        "Realizamos un 'JOIN' para unir los precios de los art√≠culos con sus nombres traducidos al ingl√©s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "T_lhZpU7qAvP",
        "outputId": "b684696b-f199-40f7-dab9-7b88757e828c"
      },
      "outputs": [],
      "source": [
        "inspector = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
        "print(\"Tablas disponibles\")\n",
        "print(inspector)\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "      t.product_category_name_english AS categoria,\n",
        "      SUM(i.price) AS ingresos_totales\n",
        "FROM items i\n",
        "JOIN products p ON i.product_id = p.product_id\n",
        "JOIN translation t ON p.product_category_name = t.product_category_name\n",
        "GROUP BY 1\n",
        "ORDER BY ingresos_totales DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "df_top_revenue = pd.read_sql_query(query, conn)\n",
        "df_top_revenue\n",
        "\n",
        "# Graficamos\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(data=df_top_revenue, x='ingresos_totales', y='categoria',palette='viridis')\n",
        "plt.title('Top 10 categor√≠as por facturaci√≥n', fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asnx0Y0eclI-"
      },
      "source": [
        "## Paso 4: An√°lisis de Lealtad y Recurrencia\n",
        "**Pregunta de Negocio:** ¬øNuestros clientes compran m√°s de una vez o somos una plataforma de \"una sola compra\"?\n",
        "Este an√°lisis es vital antes de entrenar un modelo de Machine Learning, ya que determina si debemos enfocarnos en **Adquisici√≥n** o en **Retenci√≥n**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "pvOoOMB_AIr5",
        "outputId": "b22022fc-8fef-493f-d731-7b861e36ab4f"
      },
      "outputs": [],
      "source": [
        "# Query para contar pedidos por cliente √∫nico\n",
        "\n",
        "query_frecuencia = \"\"\"\n",
        "SELECT\n",
        "      customer_unique_id,\n",
        "      COUNT(order_id) AS n_pedidos\n",
        "FROM orders o\n",
        "JOIN (SELECT customer_id , customer_unique_id FROM customers) c\n",
        "  ON o.customer_id = c.customer_id\n",
        "GROUP BY 1\n",
        "ORDER BY n_pedidos DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_frecuencia = pd.read_sql_query(query_frecuencia, conn)\n",
        "df_frecuencia\n",
        "\n",
        "# Visualizaci√≥n\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(data=df_frecuencia[df_frecuencia['n_pedidos']< 5], x='n_pedidos', palette='Blues')\n",
        "plt.title('Distribuci√≥n de pedidos por cliente', fontsize=15)\n",
        "plt.xlabel('N√∫mero de pedidos', fontsize=12)\n",
        "plt.ylabel('N√∫mero de clientes', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Mostramos el porcentaje de clientes que repiten\n",
        "\n",
        "repiten = (df_frecuencia['n_pedidos']>1).sum()\n",
        "total_clientes = len(df_frecuencia)\n",
        "print(f\"El {(repiten/total_clientes)*100:.2f}% de clientes han comprado m√°s de una vez. \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrf8b-RNqvD-"
      },
      "source": [
        "## Paso 5: Preparaci√≥n de Datos para IA (Modelo RFM)\n",
        "Para que un modelo de Machine Learning (como K-Means) pueda segmentar clientes, necesitamos transformar los datos transaccionales en variables num√©ricas. Utilizaremos la metodolog√≠a **RFM**:\n",
        "\n",
        "1. **Recency (Recencia):** ¬øHace cu√°ntos d√≠as fue su √∫ltima compra?\n",
        "2. **Frequency (Frecuencia):** ¬øCu√°ntas veces ha comprado?\n",
        "3. **Monetary (Monetario):** ¬øCu√°nto dinero ha gastado en total?\n",
        "\n",
        "*Nota T√©cnica:* Como el dataset es hist√≥rico, calcularemos la \"Recencia\" tomando como referencia el d√≠a posterior a la √∫ltima compra registrada en toda la base de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "4z9mDh7LCObR",
        "outputId": "cf71060a-d51f-4bf0-d223-3a4b5ea449f2"
      },
      "outputs": [],
      "source": [
        "# Query para crear nuestra \"Feature Table\" para la IA\n",
        "\n",
        "query_rfm = \"\"\"\n",
        "WITH facturacion_por_pedido AS (\n",
        "  SELECT order_id , SUM(price) as valor_pedido\n",
        "  FROM items\n",
        "  GROUP BY 1\n",
        "),\n",
        "datos_clientes AS(\n",
        "  SELECT\n",
        "    c.customer_unique_id,\n",
        "    o.order_purchase_timestamp,\n",
        "    f.valor_pedido\n",
        "  FROM orders o\n",
        "  JOIN customers c ON o.customer_id = c.customer_id\n",
        "  JOIN facturacion_por_pedido f ON o.order_id = f.order_id\n",
        "  WHERE o.order_status = 'delivered'\n",
        ")\n",
        "SELECT\n",
        "  customer_unique_id,\n",
        "  CAST((julianday((SELECT MAX(order_purchase_timestamp) FROM orders)) - julianday(MAX(order_purchase_timestamp))) AS INT) AS recencia,\n",
        "  COUNT(*) AS frecuencia,\n",
        "  SUM(valor_pedido) AS monetario\n",
        "FROM datos_clientes\n",
        "GROUP BY 1;\n",
        "\"\"\"\n",
        "\n",
        "df_rfm = pd.read_sql_query(query_rfm, conn)\n",
        "df_rfm\n",
        "\n",
        "# Mostramos las primeras filas\n",
        "print(\"Tabla RFM lista para el modelo de ML:\")\n",
        "display(df_rfm.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQOL_jcqvC9k"
      },
      "source": [
        "###  An√°lisis de Outliers en RFM\n",
        "Antes de entrenar un modelo de IA (K-Means), debemos observar la distribuci√≥n de nuestros datos. Los modelos de clustering son muy sensibles a los valores at√≠picos (outliers), como esos clientes que gastan 100 veces m√°s que el promedio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "IZ4wZEsAsMWt",
        "outputId": "2856dbba-88df-4ee4-ab43-e3acdc1d92fa"
      },
      "outputs": [],
      "source": [
        "# Graficamos la relaci√≥n entre Frecuencia y Monetario\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df_rfm, x='frecuencia', y='monetario', alpha=0.5, color='teal')\n",
        "plt.title('Relaci√≥n Frecuencia vs Monetario (Detecci√≥n de Outliers)', fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "# Resumen estad√≠stico\n",
        "print(df_rfm[['recencia', 'frecuencia', 'monetario']].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcGVAajbmGbj"
      },
      "source": [
        "## Conceptos Fundamentales: El Modelo RFM\n",
        "\n",
        "Antes de aplicar Inteligencia Artificial, debemos entender las dimensiones que definen el comportamiento de nuestros clientes. El modelo **RFM** es el est√°ndar de la industria para segmentaci√≥n:\n",
        "\n",
        "1. **Recency (Recencia - R):** D√≠as transcurridos desde la √∫ltima compra.\n",
        "   * *L√≥gica:* Un cliente que compr√≥ recientemente tiene la marca fresca en su mente y es m√°s probable que responda a promociones.\n",
        "2. **Frequency (Frecuencia - F):** Cantidad total de pedidos realizados.\n",
        "   * *L√≥gica:* Mide la lealtad. Un cliente frecuente conf√≠a en la plataforma.\n",
        "3. **Monetary (Monetario - M):** Valor total gastado por el cliente.\n",
        "   * *L√≥gica:* Mide el valor econ√≥mico. Ayuda a diferenciar entre clientes de alto ticket y compradores ocasionales de bajo costo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mHOSYFgfqHE"
      },
      "source": [
        "## Paso 6: Escalado de Caracter√≠sticas (Feature Scaling)\n",
        "Los algoritmos de ML basados en distancias (como **K-Means**, que usaremos para segmentar clientes) son muy sensibles a la escala de los datos.\n",
        "\n",
        "**El Problema:**\n",
        "* La **Recencia** tiene valores entre 0  y 700+\n",
        "* La **Frecuencia** tiene valores mayoritariamente entre 1 y 5\n",
        "* El **Monetario** tiene valores de 0 a 10.000+\n",
        "\n",
        "Si no escalamos, el algoritmo pensar√° que el dinero es miles de veces m√°s importante que\n",
        "la frecuencia solo porque el n√∫mero es m√°s grande. Para solucionarlo, usaremos **StandardScaler**, que ajusta los datos para que tengan una media de 0 y una desviaci√≥n est√°ndar de 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Y914AVs_vGuw",
        "outputId": "5a8dd5e7-a158-41af-bb3e-0749bf9e48c4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1 Seleccionamos solo las columnas n√∫mericas que usaremos para la IA\n",
        "features = ['recencia', 'frecuencia', 'monetario']\n",
        "data_to_scale = df_rfm[features]\n",
        "\n",
        "# 2 Inicializamos el escalador\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 3 Ajustamos y transformamos los datos\n",
        "# fit_transform aprende la media y varianza y luego escala los datos\n",
        "rfm_scaled = scaler.fit_transform(data_to_scale)\n",
        "\n",
        "# 4 Convertimos de nuevo a un DataFrame para que sea m√°s f√°cil de leer\n",
        "df_rfm_scaled = pd.DataFrame(rfm_scaled, columns=features)\n",
        "df_rfm_scaled.head()\n",
        "\n",
        "# A√±adimos el ID del cliente de nuevo para no perderlo\n",
        "df_rfm_scaled['customer_unique_id'] = df_rfm.reset_index()['customer_unique_id']\n",
        "\n",
        "print(\"Datos normalizados. Ahora todos los valores est√°n en una escala similar.\")\n",
        "display(df_rfm_scaled.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-KNnLNDlK0F"
      },
      "source": [
        "###  Insight T√©cnico\n",
        "Al aplicar `StandardScaler`, la f√≥rmula que se ejecuta detr√°s es:\n",
        "$$z = \\frac{(x - \\mu)}{\\sigma}$$\n",
        "Donde $x$ es el valor original, $\\mu$ es el promedio y $\\sigma$ es la desviaci√≥n est√°ndar.\n",
        "Ahora, un \"1\" en frecuencia pesa lo mismo que un \"1\" en dinero para los ojos de nuestra IA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsPowDjjmOrl"
      },
      "source": [
        "##  ¬øC√≥mo interpretar los valores normalizados?\n",
        "\n",
        "Tras aplicar `StandardScaler`, los datos se transforman a una escala de **Z-Score** donde el **0** representa el promedio de toda la base de datos.\n",
        "\n",
        "| Variable | Valor Positivo (+) | Valor Negativo (-) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Recency** | Compr√≥ hace **m√°s** d√≠as que el promedio (Cliente inactivo). | Compr√≥ hace **menos** d√≠as que el promedio (Cliente reciente). **¬°Es bueno!** |\n",
        "| **Frequency** | Compr√≥ **m√°s** veces que el promedio (Leal). | Compr√≥ **menos** veces que el promedio (Ocasional). |\n",
        "| **Monetary** | Gast√≥ **m√°s** dinero que el promedio (Alto Valor). | Gast√≥ **menos** dinero que el promedio (Bajo Valor). |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa2ibPAmUJB"
      },
      "source": [
        "## Paso 7: Determinaci√≥n del N√∫mero de Clusters (M√©todo del Codo)\n",
        "El algoritmo **K-Means** es un aprendizaje no supervisado que agrupa puntos por cercania. Sin embargo, requiere que nosotros definamos el n√∫mero de grupos (`k`).\n",
        "\n",
        "Para no elegir un n√∫mero al azar, utilizamos el **M√©todo del Codo (Elbow Method)**:\n",
        "1. Probamos el algoritmo con diferentes valores de `k` (del 1 al 10).\n",
        "2. Calculamos la **Inercia** (o WCSS): la suma de las distancias al cuadrado de cada punto al centro del grupo.\n",
        "3. Buscamos el punto donde la ca√≠da de la inercia se suaviza (el \"codo\"), lo que indica que a√±adir m√°s grupos no aporta una mejora significativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "AHGEMuhbklCS",
        "outputId": "f38c9909-7856-4a58-bb4e-04cab0138f39"
      },
      "outputs": [],
      "source": [
        "from matplotlib import lines\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1 Preparar lista para almacenar la inercia (WCSS)\n",
        "\n",
        "wcss=[]\n",
        "\n",
        "# 2 Bucle para probar k del 1 al 10\n",
        "# Usamos random_state=42 para que los resultados sean siempre iguales\n",
        "for i in range(1,11):\n",
        "  kmeans = KMeans(n_clusters=i, init='k-means++' ,random_state=42, n_init=10)\n",
        "  kmeans.fit(rfm_scaled)\n",
        "  wcss.append(kmeans.inertia_)\n",
        "\n",
        "# 3 Visualizamos los resultados\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1,11),wcss, marker='o',color='indigo', linestyle='--')\n",
        "plt.title('M√©todo del Codo (Elbow Method)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('N√∫mero de clusters (k)',fontsize=12)\n",
        "plt.ylabel('WCSS',fontsize=12)\n",
        "plt.xticks(range(1,11))\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQqoIclv5oqe"
      },
      "source": [
        "## Paso 8: Entrenamiento del Modelo y Asignaci√≥n de Clusters\n",
        "Con el valor √≥ptimo de $k=5$, procedemos a entrenar el algoritmo **K-Means**.\n",
        "Este proceso agrupar√° a los clientes en 5 segmentos distintos basados en la proximidad de sus vectores RFM normalizados.\n",
        "\n",
        "Posteriormente, devolveremos estas etiquetas a nuestro DataFrame original para analizar las caracter√≠sticas de cada grupo en valores reales (R$, d√≠as, unidades)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "HZ03MJRMoBZr",
        "outputId": "206302c8-5bdf-43dc-b100-742482b284fc"
      },
      "outputs": [],
      "source": [
        "# 1 Configuramos el modelo con k=5\n",
        "kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42, n_init=10)\n",
        "\n",
        "# 2 Entrenamos y predecimos las etiquetas\n",
        "clusters = kmeans.fit_predict(rfm_scaled)\n",
        "\n",
        "# 3 A√±adimos la etiqueta de cluster al DataFrame original (el de los datos reales)\n",
        "df_rfm['cluster'] = clusters\n",
        "\n",
        "# 4 Verificamos cu√°ntos clientes cayeron en cada grupo\n",
        "print(\"Cantidad de clientes por segmento:\")\n",
        "print(df_rfm['cluster'].value_counts())\n",
        "\n",
        "display(df_rfm.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL8ymRkCmSdu"
      },
      "source": [
        "## Paso 9: Perfilamiento de los Segmentos\n",
        "Para enternder qu√© significa cada cluster, calculamos los valores promedio de **Recencia, Frecuencia y Monetario** para cada grupo.\n",
        "\n",
        "Esto nos permitir√° ponerles \"nombre y apellido\" (ej. Campeones, Clientes Perdidos, etc...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "2vyLHVXy6x-x",
        "outputId": "ba059804-790e-4e2f-bea3-b3d47e9cd8ca"
      },
      "outputs": [],
      "source": [
        "# Agrupamos por cluster  y calculamos la media de las variables originales\n",
        "\n",
        "perfil_clusters = df_rfm.groupby('cluster').agg({\n",
        "    'recencia':'mean',\n",
        "    'frecuencia':'mean',\n",
        "    'monetario':'mean',\n",
        "    'customer_unique_id':'count'\n",
        "}).rename(columns={'customer_unique_id':'n_clientes'}).sort_values('monetario',ascending=False)\n",
        "\n",
        "\n",
        "# Formateamos para que sea legible\n",
        "pd.options.display.float_format= '{:,.2f}'.format\n",
        "print(\"Perfil promedio de cada segmento:\")\n",
        "display(perfil_clusters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PYdG_ubpKv6"
      },
      "source": [
        "## Paso 10: Definici√≥n de Estrategia por Segmento\n",
        "\n",
        "Tras analizar los promedios, definimos las siguientes categor√≠as para el negocio:\n",
        "| Cluster | Nombre sugerido | Perfil | Estrategia de IA |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **0** | **Grandes Compradores** | Gasto muy alto, pero inactivos hace meses. | Campa√±a de \"Win-back\" con productos de lujo. |\n",
        "| **4** | **Clientes Fieles** | Los √∫nicos que repiten compra (Freq > 2). | Programa de puntos y fidelizaci√≥n. |\n",
        "| **2** | **Nuevas Promesas** | Compraron hace poco (Recencia baja). | Recomendaci√≥n de productos (Cross-selling). |\n",
        "| **1** | **Clientes Tibios** | Compraron hace un a√±o, gasto bajo. | Descuentos de reactivaci√≥n por tiempo limitado. |\n",
        "| **3** | **Inactivos / Fugados** | No compran hace m√°s de 500 d√≠as. | No gastar presupuesto de marketing aqu√≠. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpoq_kl3rbK6"
      },
      "source": [
        "## Paso 10.1: Visualizaci√≥n Espacial de Clientes\n",
        "Para validar f√≠sicamente la segmentaci√≥n de nuestras IA, proyectamos a los clientes en un espacio de tres dimensiones.\n",
        "\n",
        "* **Eje X (Recencia):** Muestra qu√© tan reciente es la compra.\n",
        "* **Eje Y (Frecuencia):** Muestra la lealtad.\n",
        "* **Eje Z (Monetario):** Muestra el valor econ√≥mico.\n",
        "\n",
        "Esta gr√°fica interactiva permite identificar los \"bordes\" de cada cluster y entender c√≥mo el algoritmo K-Means agrup√≥ a los usuarios por su comportamiento similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "CWKjWSe7pYBM",
        "outputId": "f76491cb-1158-40cb-d10b-d63e2877c506"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Creamos el gr√°fico 3D interactivo\n",
        "fig = px.scatter_3d(\n",
        "    df_rfm,\n",
        "    x='recencia',\n",
        "    y='frecuencia',\n",
        "    z='monetario',\n",
        "    color='cluster',        # Diferencia los grupos por color\n",
        "    title='Segmentaci√≥n 3D de Clientes (Modelo RFM)',\n",
        "    labels={\n",
        "        'recencia':'D√≠as (R)',\n",
        "        'frecuencia':'Pedidos (F)',\n",
        "        'monetario':'Gasto R$ (M)'\n",
        "    },\n",
        "    opacity=0.6,                      # Transparencia para ver puntos solapados\n",
        "    color_continuous_scale='Viridis'  # Paleta de colores pro\n",
        ")\n",
        "\n",
        "# Ajustamos el tama√±o y m√°rgenes\n",
        "fig.update_layout(\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    scene=dict(\n",
        "        xaxis_title='Recencia',\n",
        "        yaxis_title='Frecuencia',\n",
        "        zaxis_title='Monetario'\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgJvlxa0uFWZ"
      },
      "source": [
        "## Paso 11: IA Generativa para Marketing Personalizado\n",
        "En un entorno de producci√≥n moderno, no escribimos los correos a mano. Utilizamos la etiqueta del **Cluster** generada por nuestra IA de ML para alimentar un  **Prompt** que una IA Generativa (LLM) usar√° para crear mensajes √∫nicos.\n",
        "\n",
        "A continuaci√≥n, creamos una funci√≥n que asigna la \"Instrucci√≥n de Marketing\" basada en la predicci√≥n del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "7eMxnCrot4ti",
        "outputId": "5b47df39-45a7-4c26-cf06-5d97e6a2360f"
      },
      "outputs": [],
      "source": [
        "# Definimos las instrucciones que le dar√≠amos a una API de IA\n",
        "# para cada tipo de cliente identificado por K-Means\n",
        "\n",
        "def asignar_instruccion_ia(cluster):\n",
        "  instrucciones= {\n",
        "      0:\"Generar un correo de lujo invitando a conocer la nueva colecci√≥n premium.\",\n",
        "      4:\"Agradecer por ser parte del 3% de clientes exclusivos y ofrecer env√≠o gratis.\",\n",
        "      2:\"Dar la bienvenida y sugerir productos basados en su primera categor√≠a de compra.\",\n",
        "      1:\"Enviar un recordatorio de 'Te extra√±amos' con un 10% de descuento.\",\n",
        "      3:\"Marcar como inactivo en el CRM para optimizar costos de env√≠o de emails.\"\n",
        "  }\n",
        "  return instrucciones.get(cluster,\"Enviar comunicaci√≥n est√°ndar.\")\n",
        "\n",
        "# Aplicamos la l√≥gica a nuestro DataFrame\n",
        "df_rfm['accion_ia'] = df_rfm['cluster'].apply(asignar_instruccion_ia)\n",
        "\n",
        "# Visualizamos el resultado final\n",
        "print(\"Pipeline de IA completado\")\n",
        "display(df_rfm[['customer_unique_id','cluster','accion_ia']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYC9PgJQDZvu"
      },
      "source": [
        "# üèÅ RESUMEN EJECUTIVO: Olist-Analysis - Inteligencia de Clientes E-commerce\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ El Problema de Negocio\n",
        "**Olist** presentaba una base de datos masiva (+100k registros) pero sin segmentar, con una alta dependencia de adquisici√≥n de clientes nuevos. El objetivo estrat√©gico fue transformar estos datos transaccionales en **perfiles de comportamiento** (Customer Personas) para optimizar el presupuesto de marketing, personalizar la comunicaci√≥n y aumentar la retenci√≥n de usuarios.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Stack Tecnol√≥gico\n",
        "* **Gesti√≥n de Datos:** SQL (SQLite) para el cruce eficiente de tablas relacionales.\n",
        "* **Procesamiento:** Python (Pandas, NumPy).\n",
        "* **Machine Learning:** Scikit-Learn (StandardScaler, KMeans).\n",
        "* **Visualizaci√≥n:** Seaborn, Plotly (3D Interactivo).\n",
        "* **Estrategia:** IA Generativa (Prompt Engineering para automatizaci√≥n de campa√±as).\n",
        "\n",
        "---\n",
        "\n",
        "## üó∫Ô∏è Mapa de Ruta: Los 11 Hitos Logrados\n",
        "\n",
        "| Fase | Hito | Descripci√≥n T√©cnica |\n",
        "| :--- | :--- | :--- |\n",
        "| **Ingenier√≠a de Datos** | 1-3. SQL Joins | Conexi√≥n de `orders`, `items`, `customers` y `products` usando Aliases (`AS`). |\n",
        "| **An√°lisis (EDA)** | 4-5. Retenci√≥n | Identificaci√≥n de una tasa de recurrencia cr√≠tica del **3.12%**. |\n",
        "| **ML Prep** | 6. RFM | Ingenier√≠a de caracter√≠sticas: **Recency, Frequency, Monetary**. |\n",
        "| **ML Prep** | 7. Scaling | Normalizaci√≥n de datos con `StandardScaler` para equilibrar magnitudes. |\n",
        "| **Modelado** | 8. Elbow Method | Validaci√≥n de $k=5$ clusters mediante el an√°lisis de Inercia (WCSS). |\n",
        "| **Modelado** | 9. Clustering | Entrenamiento de **K-Means** y asignaci√≥n de etiquetas autom√°ticas. |\n",
        "| **Estrategia** | 10. Naming | Traducci√≥n de clusters a Personas: *Champions, At Risk, New Promises*. |\n",
        "| **Estrategia** | 11. AI Logic | Creaci√≥n de un pipeline de comunicaci√≥n personalizada v√≠a Prompting. |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Conceptos Senior para Entrevistas\n",
        "\n",
        "* **¬øPor qu√© SQL y no solo Pandas?** Para simular un entorno productivo de *Data Warehouse*, garantizando la escalabilidad del an√°lisis y optimizando el uso de memoria RAM.\n",
        "* **La l√≥gica del RFM:** * **$R$ (Recencia):** A menor valor (negativo en escala normalizada), m√°s fresco es el cliente.\n",
        "    * **$F$ (Frecuencia):** Mide la lealtad mediante el volumen de compras repetidas.\n",
        "    * **$M$ (Monetario):** Indica el valor de vida del cliente (*LTV*).\n",
        "* **Normalizaci√≥n Z-Score:** $$z = \\frac{(x - \\mu)}{\\sigma}$$\n",
        "    Donde $\\mu$ es la media y $\\sigma$ la desviaci√≥n est√°ndar. Este paso es crucial para que el algoritmo de IA no se sesgue por variables con n√∫meros grandes (como el dinero) sobre las peque√±as (frecuencia).\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
